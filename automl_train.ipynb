{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ячейках ниже происходит обучение композиции моделей машинного обучения с помощью фреймворка lightautoml. Для его корректной работы необходимо использовать python 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "# from ipyplot import plot_images\n",
    "# import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\n",
    "from lightautoml.tasks import Task\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from lightautoml.report.report_deco import ReportDeco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгружаем заранее сформированный обучающий датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"meta_wag_final.csv\", index_col=0)\n",
    "data.replace(['no_data'], -1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем часть признаков + создаем два набора обуччающих данных, где в одном случае таргет на месяц, а в другом случае таргет на 10 дней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m = data.drop(columns=[\"target_day\", \"wagnum\", 'month', 'date_build', 'srok_sl', 'model', 'date_pl_rem', 'plan_date'])\n",
    "data_d = data.drop(columns=[\"target_month\", \"wagnum\", 'month', 'date_build', 'srok_sl', 'model', 'date_pl_rem', 'plan_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговый набор признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ost_prob_x', 'manage_type', 'rod_id', 'reestr_state', 'target_month',\n",
       "       'prob_diff', 'gruz', 'cnsi_gruz_capacity', 'cnsi_volumek', 'tara',\n",
       "       'zavod_build', 'cnsi_probeg_dr', 'cnsi_probeg_kr', 'kuzov', 'telega',\n",
       "       'tormoz', 'tipvozd', 'tippogl', 'norma_km', 'ownertype', 'lifespan',\n",
       "       'lefttime', 'kod_vrab_x', 'neis1_kod', 'neis2_kod', 'neis3_kod',\n",
       "       'mod1_kod', 'mod2_kod', 'mod3_kod', 'mod4_kod', 'mod5_kod', 'mod6_kod',\n",
       "       'mod7_kod', 'road_id_send', 'gr_probeg', 'por_probeg', 'st_id_send_x',\n",
       "       'rem_count', 'date_kap', 'date_dep', 'kod_vrab_y', 'id_road_disl',\n",
       "       'st_id_dest', 'id_road_dest', 'st_id_send_y', 'id_road_send',\n",
       "       'ost_prob_y', 'isload', 'fr_id', 'last_fr_id', 'distance', 'days_load',\n",
       "       'fr_changes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_m.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 1\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "TIMEOUT = 300\n",
    "TARGET_NAME_MONTH = 'target_month'\n",
    "TARGET_NAME_DAY = 'target_day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаются 2 модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-ая модель - таргет на месяц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = {\n",
    "    'target': TARGET_NAME_MONTH,\n",
    "    \n",
    "}\n",
    "\n",
    "task = Task(\n",
    "        name = 'binary',\n",
    "        metric = lambda y_true, y_pred: f1_score(y_true, (y_pred > 0.5)*1)\n",
    ")\n",
    "\n",
    "automl_m = TabularAutoML(\n",
    "    task = task,\n",
    "    timeout = TIMEOUT,\n",
    "    cpu_limit = N_THREADS,\n",
    "    reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:06:26] Stdout logging level is DEBUG.\n",
      "[09:06:26] Task: binary\n",
      "\n",
      "[09:06:26] Start automl preset with listed constraints:\n",
      "[09:06:26] - time: 300.00 seconds\n",
      "[09:06:26] - CPU: 1 cores\n",
      "[09:06:26] - memory: 16 GB\n",
      "\n",
      "[09:06:26] \u001b[1mTrain data shape: (194615, 53)\u001b[0m\n",
      "\n",
      "[09:06:35] Feats was rejected during automatic roles guess: []\n",
      "[09:06:36] Layer \u001b[1m1\u001b[0m train process start. Time left 290.02 secs\n",
      "[09:06:44] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[09:06:44] Training params: {'tol': 1e-06, 'max_iter': 100, 'cs': [1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000], 'early_stopping': 2, 'categorical_idx': [57, 58, 59, 60, 61, 62, 63], 'embed_sizes': array([30, 30, 23, 16, 16, 21, 11]), 'data_size': 64}\n",
      "[09:06:44] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[09:06:46] Linear model: C = 1e-05 score = 0.0\n",
      "[09:06:46] Linear model: C = 5e-05 score = 0.012448132780082987\n",
      "[09:06:47] Linear model: C = 0.0001 score = 0.09467455621301775\n",
      "[09:06:48] Linear model: C = 0.0005 score = 0.16448598130841122\n",
      "[09:06:49] Linear model: C = 0.001 score = 0.18331805682859761\n",
      "[09:06:51] Linear model: C = 0.005 score = 0.2990420658059142\n",
      "[09:06:53] Linear model: C = 0.01 score = 0.36712328767123287\n",
      "[09:06:54] Linear model: C = 0.05 score = 0.4406657018813314\n",
      "[09:06:54] Linear model: C = 0.1 score = 0.4406657018813314\n",
      "[09:06:54] Linear model: C = 0.5 score = 0.4406657018813314\n",
      "[09:06:54] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[09:06:56] Linear model: C = 1e-05 score = 0.0\n",
      "[09:06:57] Linear model: C = 5e-05 score = 0.011410788381742738\n",
      "[09:06:57] Linear model: C = 0.0001 score = 0.10034431874077718\n",
      "[09:06:58] Linear model: C = 0.0005 score = 0.17326502095947835\n",
      "[09:06:59] Linear model: C = 0.001 score = 0.1924839596700275\n",
      "[09:07:01] Linear model: C = 0.005 score = 0.29679595278246207\n",
      "[09:07:02] Linear model: C = 0.01 score = 0.36320568495854716\n",
      "[09:07:05] Linear model: C = 0.05 score = 0.450937950937951\n",
      "[09:07:05] Linear model: C = 0.1 score = 0.450937950937951\n",
      "[09:07:05] Linear model: C = 0.5 score = 0.450937950937951\n",
      "[09:07:05] Time limit exceeded after calculating fold 1\n",
      "\n",
      "[09:07:05] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.4458092485549133\u001b[0m\n",
      "[09:07:05] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[09:07:05] Time left 260.65 secs\n",
      "\n",
      "[09:07:06] Training until validation scores don't improve for 100 rounds\n",
      "[09:07:10] [100]\tvalid's binary_logloss: 0.0840573\tvalid's Opt metric: 0.606343\n",
      "[09:07:15] [200]\tvalid's binary_logloss: 0.0835558\tvalid's Opt metric: 0.605166\n",
      "[09:07:16] Early stopping, best iteration is:\n",
      "[130]\tvalid's binary_logloss: 0.0834244\tvalid's Opt metric: 0.609636\n",
      "[09:07:16] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[09:07:26] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[09:07:26] Training params: {'task': 'train', 'learning_rate': 0.04, 'num_leaves': 128, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 1, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 2000, 'early_stopping_rounds': 100, 'random_state': 42}\n",
      "[09:07:26] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[09:07:27] Training until validation scores don't improve for 100 rounds\n",
      "[09:07:32] [100]\tvalid's binary_logloss: 0.0848316\tvalid's Opt metric: 0.60665\n",
      "[09:07:38] [200]\tvalid's binary_logloss: 0.0828405\tvalid's Opt metric: 0.608535\n",
      "[09:07:41] Early stopping, best iteration is:\n",
      "[159]\tvalid's binary_logloss: 0.0829268\tvalid's Opt metric: 0.612674\n",
      "[09:07:41] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[09:07:43] Training until validation scores don't improve for 100 rounds\n",
      "[09:07:47] [100]\tvalid's binary_logloss: 0.0841892\tvalid's Opt metric: 0.602886\n",
      "[09:07:52] [200]\tvalid's binary_logloss: 0.0824713\tvalid's Opt metric: 0.611094\n",
      "[09:07:57] [300]\tvalid's binary_logloss: 0.082877\tvalid's Opt metric: 0.609043\n",
      "[09:07:58] Early stopping, best iteration is:\n",
      "[225]\tvalid's binary_logloss: 0.0824285\tvalid's Opt metric: 0.608696\n",
      "[09:07:59] Time limit exceeded after calculating fold 1\n",
      "\n",
      "[09:07:59] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.6106752807260422\u001b[0m\n",
      "[09:07:59] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[09:07:59] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[09:07:59] Training params: {'task_type': 'CPU', 'thread_count': 1, 'random_seed': 42, 'num_trees': 2000, 'learning_rate': 0.045, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'boost_from_average': True, 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': 100, 'allow_writing_files': False}\n",
      "[09:07:59] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m =====\n",
      "[09:08:00] 0:\tlearn: 0.6003351\ttest: 0.6003633\tbest: 0.6003633 (0)\ttotal: 64.9ms\tremaining: 2m 9s\n",
      "[09:08:04] 100:\tlearn: 0.0865277\ttest: 0.0897789\tbest: 0.0897789 (100)\ttotal: 4.7s\tremaining: 1m 28s\n",
      "[09:08:09] 200:\tlearn: 0.0835469\ttest: 0.0877971\tbest: 0.0877971 (200)\ttotal: 9.22s\tremaining: 1m 22s\n",
      "[09:08:13] 300:\tlearn: 0.0814850\ttest: 0.0868094\tbest: 0.0868019 (299)\ttotal: 13.7s\tremaining: 1m 17s\n",
      "[09:08:18] 400:\tlearn: 0.0797287\ttest: 0.0864252\tbest: 0.0864252 (400)\ttotal: 18.1s\tremaining: 1m 12s\n",
      "[09:08:22] 500:\tlearn: 0.0782344\ttest: 0.0861265\tbest: 0.0861265 (500)\ttotal: 22.7s\tremaining: 1m 7s\n",
      "[09:08:27] 600:\tlearn: 0.0768463\ttest: 0.0859204\tbest: 0.0859202 (597)\ttotal: 27.3s\tremaining: 1m 3s\n",
      "[09:08:31] 700:\tlearn: 0.0755397\ttest: 0.0856438\tbest: 0.0856438 (700)\ttotal: 31.8s\tremaining: 59s\n",
      "[09:08:36] 800:\tlearn: 0.0743506\ttest: 0.0855349\tbest: 0.0855324 (799)\ttotal: 36.5s\tremaining: 54.6s\n",
      "[09:08:41] 900:\tlearn: 0.0731802\ttest: 0.0854233\tbest: 0.0854233 (900)\ttotal: 41.2s\tremaining: 50.3s\n",
      "[09:08:45] 1000:\tlearn: 0.0720968\ttest: 0.0854163\tbest: 0.0853588 (943)\ttotal: 45.8s\tremaining: 45.7s\n",
      "[09:08:50] 1100:\tlearn: 0.0710871\ttest: 0.0853485\tbest: 0.0853254 (1048)\ttotal: 50.5s\tremaining: 41.2s\n",
      "[09:08:55] 1200:\tlearn: 0.0701279\ttest: 0.0853622\tbest: 0.0852987 (1116)\ttotal: 55.3s\tremaining: 36.8s\n",
      "[09:08:55] Stopped by overfitting detector  (100 iterations wait)\n",
      "[09:08:55] bestTest = 0.08529870168\n",
      "[09:08:55] bestIteration = 1116\n",
      "[09:08:55] Shrink model to first 1117 iterations.\n",
      "[09:08:56] Time limit exceeded after calculating fold 0\n",
      "\n",
      "[09:08:56] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.5949970220369267\u001b[0m\n",
      "[09:08:56] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[09:08:56] Time left 149.99 secs\n",
      "\n",
      "[09:08:56] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[09:08:56] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[09:08:56] Blending: optimization starts with equal weights and score \u001b[1m0.5963855421686747\u001b[0m\n",
      "[09:08:56] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.6110004596292324\u001b[0m, weights = \u001b[1m[0.06739787 0.5506361  0.38196602]\u001b[0m\n",
      "[09:08:57] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.6110004596292324\u001b[0m, weights = \u001b[1m[0.06739787 0.5506361  0.38196602]\u001b[0m\n",
      "[09:08:57] Blending: no score update. Terminated\n",
      "\n",
      "[09:08:57] \u001b[1mAutoml preset training completed in 151.07 seconds\u001b[0m\n",
      "\n",
      "[09:08:57] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.06740 * (2 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.55064 * (2 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.38197 * (1 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_pred = automl_m.fit_predict(\n",
    "    data_m,\n",
    "    roles = roles,\n",
    "    verbose=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - ая модель таргет на 10 дней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:08:57] Stdout logging level is DEBUG.\n",
      "[09:08:57] Task: binary\n",
      "\n",
      "[09:08:57] Start automl preset with listed constraints:\n",
      "[09:08:57] - time: 300.00 seconds\n",
      "[09:08:57] - CPU: 1 cores\n",
      "[09:08:57] - memory: 16 GB\n",
      "\n",
      "[09:08:57] \u001b[1mTrain data shape: (194615, 53)\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:09:06] Feats was rejected during automatic roles guess: []\n",
      "[09:09:06] Layer \u001b[1m1\u001b[0m train process start. Time left 290.78 secs\n",
      "[09:09:14] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[09:09:14] Training params: {'tol': 1e-06, 'max_iter': 100, 'cs': [1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000], 'early_stopping': 2, 'categorical_idx': [57, 58, 59, 60, 61, 62], 'embed_sizes': array([30, 30, 30, 23, 16, 16]), 'data_size': 63}\n",
      "[09:09:14] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[09:09:15] Linear model: C = 1e-05 score = 0.0\n",
      "[09:09:16] Linear model: C = 5e-05 score = 0.0\n",
      "[09:09:16] Linear model: C = 0.0001 score = 0.028125\n",
      "[09:09:17] Linear model: C = 0.0005 score = 0.27319587628865977\n",
      "[09:09:17] Linear model: C = 0.001 score = 0.31396786155747836\n",
      "[09:09:18] Linear model: C = 0.005 score = 0.3444976076555024\n",
      "[09:09:18] Linear model: C = 0.01 score = 0.3444976076555024\n",
      "[09:09:20] Linear model: C = 0.05 score = 0.3440094899169632\n",
      "[09:09:20] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[09:09:22] Linear model: C = 1e-05 score = 0.0\n",
      "[09:09:22] Linear model: C = 5e-05 score = 0.0\n",
      "[09:09:23] Linear model: C = 0.0001 score = 0.02190923317683881\n",
      "[09:09:23] Linear model: C = 0.0005 score = 0.2617801047120419\n",
      "[09:09:24] Linear model: C = 0.001 score = 0.30750000000000005\n",
      "[09:09:25] Linear model: C = 0.005 score = 0.3542168674698795\n",
      "[09:09:25] Linear model: C = 0.01 score = 0.3542168674698795\n",
      "[09:09:27] Linear model: C = 0.05 score = 0.3696682464454976\n",
      "[09:09:27] Linear model: C = 0.1 score = 0.3696682464454976\n",
      "[09:09:27] Linear model: C = 0.5 score = 0.3696682464454976\n",
      "[09:09:27] Time limit exceeded after calculating fold 1\n",
      "\n",
      "[09:09:27] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.35714285714285715\u001b[0m\n",
      "[09:09:27] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[09:09:27] Time left 269.87 secs\n",
      "\n",
      "[09:09:28] Training until validation scores don't improve for 100 rounds\n",
      "[09:09:32] [100]\tvalid's binary_logloss: 0.0390563\tvalid's Opt metric: 0.503627\n",
      "[09:09:36] [200]\tvalid's binary_logloss: 0.0397496\tvalid's Opt metric: 0.514811\n",
      "[09:09:36] Early stopping, best iteration is:\n",
      "[110]\tvalid's binary_logloss: 0.0390091\tvalid's Opt metric: 0.511294\n",
      "[09:09:37] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[09:09:46] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[09:09:46] Training params: {'task': 'train', 'learning_rate': 0.04, 'num_leaves': 128, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 1, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 1, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 2000, 'early_stopping_rounds': 100, 'random_state': 42}\n",
      "[09:09:46] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[09:09:47] Training until validation scores don't improve for 100 rounds\n",
      "[09:09:51] [100]\tvalid's binary_logloss: 0.0386544\tvalid's Opt metric: 0.521385\n",
      "[09:09:56] [200]\tvalid's binary_logloss: 0.0391431\tvalid's Opt metric: 0.522449\n",
      "[09:09:58] Early stopping, best iteration is:\n",
      "[126]\tvalid's binary_logloss: 0.0383945\tvalid's Opt metric: 0.530902\n",
      "[09:09:58] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[09:09:59] Training until validation scores don't improve for 100 rounds\n",
      "[09:10:04] [100]\tvalid's binary_logloss: 0.0345314\tvalid's Opt metric: 0.56213\n",
      "[09:10:08] [200]\tvalid's binary_logloss: 0.0343934\tvalid's Opt metric: 0.573386\n",
      "[09:10:11] Early stopping, best iteration is:\n",
      "[154]\tvalid's binary_logloss: 0.0341146\tvalid's Opt metric: 0.575049\n",
      "[09:10:12] Time limit exceeded after calculating fold 1\n",
      "\n",
      "[09:10:12] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.5534028812717338\u001b[0m\n",
      "[09:10:12] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[09:10:12] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 1.00 secs\n",
      "[09:10:13] Training until validation scores don't improve for 100 rounds\n",
      "[09:10:18] [100]\tvalid's binary_logloss: 0.0406104\tvalid's Opt metric: 0.497899\n",
      "[09:10:23] Early stopping, best iteration is:\n",
      "[79]\tvalid's binary_logloss: 0.0403952\tvalid's Opt metric: 0.486828\n",
      "[09:10:24] \u001b[1mTrial 1\u001b[0m with hyperparameters {'feature_fraction': 0.6872700594236812, 'num_leaves': 244, 'bagging_fraction': 0.8659969709057025, 'min_sum_hessian_in_leaf': 0.24810409748678125, 'reg_alpha': 2.5361081166471375e-07, 'reg_lambda': 2.5348407664333426e-07} scored 0.4868282402528977 in 0:00:11.961997\n",
      "[09:10:24] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[09:10:24] The set of hyperparameters \u001b[1m{'feature_fraction': 0.6872700594236812, 'num_leaves': 244, 'bagging_fraction': 0.8659969709057025, 'min_sum_hessian_in_leaf': 0.24810409748678125, 'reg_alpha': 2.5361081166471375e-07, 'reg_lambda': 2.5348407664333426e-07}\u001b[0m\n",
      " achieve 0.4868 AutoML Metric\n",
      "[09:10:24] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[09:10:24] Training params: {'task_type': 'CPU', 'thread_count': 1, 'random_seed': 42, 'num_trees': 2000, 'learning_rate': 0.045, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'boost_from_average': True, 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': 100, 'allow_writing_files': False}\n",
      "[09:10:24] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m =====\n",
      "[09:10:24] 0:\tlearn: 0.6011210\ttest: 0.6013899\tbest: 0.6013899 (0)\ttotal: 54.6ms\tremaining: 1m 49s\n",
      "[09:10:29] 100:\tlearn: 0.0365176\ttest: 0.0406095\tbest: 0.0406095 (100)\ttotal: 4.54s\tremaining: 1m 25s\n",
      "[09:10:33] 200:\tlearn: 0.0342194\ttest: 0.0393192\tbest: 0.0393192 (200)\ttotal: 9.02s\tremaining: 1m 20s\n",
      "[09:10:38] 300:\tlearn: 0.0327023\ttest: 0.0389280\tbest: 0.0389253 (299)\ttotal: 13.4s\tremaining: 1m 15s\n",
      "[09:10:42] 400:\tlearn: 0.0313850\ttest: 0.0386986\tbest: 0.0386934 (398)\ttotal: 18s\tremaining: 1m 11s\n",
      "[09:10:47] 500:\tlearn: 0.0301666\ttest: 0.0385279\tbest: 0.0385279 (500)\ttotal: 22.6s\tremaining: 1m 7s\n",
      "[09:10:51] 600:\tlearn: 0.0291254\ttest: 0.0384237\tbest: 0.0383833 (556)\ttotal: 27s\tremaining: 1m 2s\n",
      "[09:10:54] Stopped by overfitting detector  (100 iterations wait)\n",
      "[09:10:54] bestTest = 0.03838333817\n",
      "[09:10:54] bestIteration = 556\n",
      "[09:10:54] Shrink model to first 557 iterations.\n",
      "[09:10:54] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m =====\n",
      "[09:10:54] 0:\tlearn: 0.6014955\ttest: 0.6027750\tbest: 0.6027750 (0)\ttotal: 49ms\tremaining: 1m 38s\n",
      "[09:11:00] 100:\tlearn: 0.0379284\ttest: 0.0373381\tbest: 0.0373381 (100)\ttotal: 5.09s\tremaining: 1m 35s\n",
      "[09:11:05] 200:\tlearn: 0.0357610\ttest: 0.0363784\tbest: 0.0363730 (197)\ttotal: 10.1s\tremaining: 1m 30s\n",
      "[09:11:10] 300:\tlearn: 0.0341469\ttest: 0.0359479\tbest: 0.0359479 (300)\ttotal: 15.1s\tremaining: 1m 25s\n",
      "[09:11:15] 400:\tlearn: 0.0327553\ttest: 0.0359440\tbest: 0.0358364 (381)\ttotal: 20.2s\tremaining: 1m 20s\n",
      "[09:11:19] 500:\tlearn: 0.0316299\ttest: 0.0358162\tbest: 0.0357993 (485)\ttotal: 25.1s\tremaining: 1m 14s\n",
      "[09:11:23] Stopped by overfitting detector  (100 iterations wait)\n",
      "[09:11:23] bestTest = 0.03579933152\n",
      "[09:11:23] bestIteration = 485\n",
      "[09:11:23] Shrink model to first 486 iterations.\n",
      "[09:11:24] Time limit exceeded after calculating fold 1\n",
      "\n",
      "[09:11:24] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.4897314375987362\u001b[0m\n",
      "[09:11:24] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[09:11:24] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 1.00 secs\n",
      "[09:11:24] 0:\tlearn: 0.6020431\ttest: 0.6022007\tbest: 0.6022007 (0)\ttotal: 41.1ms\tremaining: 1m 22s\n",
      "[09:11:29] 100:\tlearn: 0.0381986\ttest: 0.0415973\tbest: 0.0415973 (100)\ttotal: 4.22s\tremaining: 1m 19s\n",
      "[09:11:33] 200:\tlearn: 0.0362585\ttest: 0.0401364\tbest: 0.0401364 (200)\ttotal: 8.4s\tremaining: 1m 15s\n",
      "[09:11:37] 300:\tlearn: 0.0352789\ttest: 0.0395648\tbest: 0.0395648 (300)\ttotal: 12.9s\tremaining: 1m 12s\n",
      "[09:11:43] 400:\tlearn: 0.0344745\ttest: 0.0392100\tbest: 0.0392100 (400)\ttotal: 18.3s\tremaining: 1m 12s\n",
      "[09:11:47] 500:\tlearn: 0.0338088\ttest: 0.0390363\tbest: 0.0390363 (500)\ttotal: 22.3s\tremaining: 1m 6s\n",
      "[09:11:51] 600:\tlearn: 0.0331969\ttest: 0.0388688\tbest: 0.0388668 (599)\ttotal: 26.6s\tremaining: 1m 1s\n",
      "[09:11:55] 700:\tlearn: 0.0326368\ttest: 0.0387121\tbest: 0.0387121 (700)\ttotal: 30.6s\tremaining: 56.7s\n",
      "[09:11:59] 800:\tlearn: 0.0321351\ttest: 0.0385618\tbest: 0.0385585 (793)\ttotal: 34.8s\tremaining: 52s\n",
      "[09:12:03] 900:\tlearn: 0.0316409\ttest: 0.0384311\tbest: 0.0384311 (900)\ttotal: 38.8s\tremaining: 47.4s\n",
      "[09:12:07] 1000:\tlearn: 0.0311972\ttest: 0.0383300\tbest: 0.0383292 (999)\ttotal: 42.9s\tremaining: 42.8s\n",
      "[09:12:11] 1100:\tlearn: 0.0307784\ttest: 0.0382695\tbest: 0.0382565 (1085)\ttotal: 47s\tremaining: 38.4s\n",
      "[09:12:16] 1200:\tlearn: 0.0303766\ttest: 0.0382300\tbest: 0.0382300 (1200)\ttotal: 51.4s\tremaining: 34.2s\n",
      "[09:12:20] 1300:\tlearn: 0.0299780\ttest: 0.0381669\tbest: 0.0381591 (1269)\ttotal: 55.7s\tremaining: 29.9s\n",
      "[09:12:24] 1400:\tlearn: 0.0296109\ttest: 0.0381459\tbest: 0.0381449 (1348)\ttotal: 59.7s\tremaining: 25.5s\n",
      "[09:12:28] 1500:\tlearn: 0.0292716\ttest: 0.0381288\tbest: 0.0381226 (1495)\ttotal: 1m 3s\tremaining: 21.2s\n",
      "[09:12:32] 1600:\tlearn: 0.0289408\ttest: 0.0380928\tbest: 0.0380849 (1591)\ttotal: 1m 8s\tremaining: 16.9s\n",
      "[09:12:36] 1700:\tlearn: 0.0286033\ttest: 0.0380872\tbest: 0.0380763 (1677)\ttotal: 1m 12s\tremaining: 12.7s\n",
      "[09:12:40] 1800:\tlearn: 0.0282692\ttest: 0.0380513\tbest: 0.0380455 (1754)\ttotal: 1m 16s\tremaining: 8.4s\n",
      "[09:12:44] 1900:\tlearn: 0.0279450\ttest: 0.0380447\tbest: 0.0380424 (1899)\ttotal: 1m 20s\tremaining: 4.17s\n",
      "[09:12:48] 1999:\tlearn: 0.0276168\ttest: 0.0380547\tbest: 0.0380408 (1902)\ttotal: 1m 24s\tremaining: 0us\n",
      "[09:12:48] bestTest = 0.03804080699\n",
      "[09:12:48] bestIteration = 1902\n",
      "[09:12:48] Shrink model to first 1903 iterations.\n",
      "[09:12:48] \u001b[1mTrial 1\u001b[0m with hyperparameters {'max_depth': 4, 'l2_leaf_reg': 3.6010467344475403, 'min_data_in_leaf': 15} scored 0.4937759336099585 in 0:01:24.897515\n",
      "[09:12:48] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[09:12:48] The set of hyperparameters \u001b[1m{'max_depth': 4, 'l2_leaf_reg': 3.6010467344475403, 'min_data_in_leaf': 15}\u001b[0m\n",
      " achieve 0.4938 AutoML Metric\n",
      "[09:12:48] Time left 68.18 secs\n",
      "\n",
      "[09:12:48] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[09:12:48] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[09:12:49] Blending: optimization starts with equal weights and score \u001b[1m0.4703296703296704\u001b[0m\n",
      "[09:12:49] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.5501002004008015\u001b[0m, weights = \u001b[1m[0.         0.90983003 0.09016994]\u001b[0m\n",
      "[09:12:50] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.5501002004008015\u001b[0m, weights = \u001b[1m[0.         0.90983003 0.09016994]\u001b[0m\n",
      "[09:12:50] Blending: no score update. Terminated\n",
      "\n",
      "[09:12:50] \u001b[1mAutoml preset training completed in 232.86 seconds\u001b[0m\n",
      "\n",
      "[09:12:50] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.90983 * (2 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.09017 * (2 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "roles = {\n",
    "    'target': 'target_day',\n",
    "}\n",
    "\n",
    "automl_d = TabularAutoML(\n",
    "    task = task,\n",
    "    timeout = TIMEOUT,\n",
    "    cpu_limit = N_THREADS,\n",
    "    reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE}\n",
    ")\n",
    "\n",
    "oof_pred_d = automl_d.fit_predict(\n",
    "    data_d,\n",
    "    roles = roles,\n",
    "    verbose=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загруаем признаки для предсказания таргетов на март"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf = pd.read_csv(\"feb_features.csv\", index_col=0)\n",
    "data_inf.replace(['no_data'], -1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inf_ = data_inf.drop(columns=[\"wagnum\", 'date_pl_rem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инференс двух моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_inf_m = automl_m.predict(data_inf_)\n",
    "pred_inf_d = automl_d.predict(data_inf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем посылку на сайт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({\n",
    "    \"wagnum\": data_inf.wagnum.values,\n",
    "    \"target_month\": (pred_inf_m.data[:, 0] > 0.5) * 1,\n",
    "    \"target_day\": (pred_inf_d.data[:, 0] > 0.5) * 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(\"y_predict_submit_example.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_df[[\"wagnum\"]]\n",
    "sub_df[\"month\"] = pd.to_datetime('2023-03-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = sub_df.merge(df_pred, how=\"left\", on=\"wagnum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wagnum</th>\n",
       "      <th>month</th>\n",
       "      <th>target_month</th>\n",
       "      <th>target_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33361</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33364</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33366</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33358</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33349</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33702</th>\n",
       "      <td>17621</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33703</th>\n",
       "      <td>25045</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33704</th>\n",
       "      <td>27156</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33705</th>\n",
       "      <td>21361</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33706</th>\n",
       "      <td>8061</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33707 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       wagnum      month  target_month  target_day\n",
       "0       33361 2023-03-01             0           0\n",
       "1       33364 2023-03-01             0           0\n",
       "2       33366 2023-03-01             0           0\n",
       "3       33358 2023-03-01             0           0\n",
       "4       33349 2023-03-01             0           0\n",
       "...       ...        ...           ...         ...\n",
       "33702   17621 2023-03-01             0           0\n",
       "33703   25045 2023-03-01             0           0\n",
       "33704   27156 2023-03-01             0           0\n",
       "33705   21361 2023-03-01             0           0\n",
       "33706    8061 2023-03-01             0           0\n",
       "\n",
       "[33707 rows x 4 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datawagon_2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
